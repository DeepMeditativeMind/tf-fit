{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_learner.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_YLAW2teP0KC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html\n",
        "!pip install git+https://github.com/fastai/fastai\n",
        "!wget www.pendar2.com/cifar10.zip\n",
        "!wget www.pendar2.com/mnist.zip\n",
        "!unzip cifar10.zip > zip-log\n",
        "!unzip mnist.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gPqGmZDEPYhm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zCDj0VkPYhs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cZNX4jb8PYhu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
        "#conf = tf.ConfigProto(gpu_options=opts)\n",
        "#tf.enable_eager_execution(config=conf)\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "tf.keras.backend.set_image_data_format('channels_first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79VMrgJcPYhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7CNXcA6fPYh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learner"
      ]
    },
    {
      "metadata": {
        "id": "N5mq1c6HPYh7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flatten_model_tf=lambda m: sum(map(flatten_model_tf,m.layers),[]) if hasattr(m, \"layers\") else [m]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OC8_99maPYh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_bn_types = (tf.keras.layers.BatchNormalization,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AG7Mu0tqPYiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.Tensor.detach = lambda x: x.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTJThuj8PYiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_batch(model, xb, yb, loss_fn=None, opt=None, cb_handler=None, metrics=None):\n",
        "    \"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\n",
        "    if cb_handler is None: cb_handler = CallbackHandler([])\n",
        "    if not is_listy(xb): xb = [xb]\n",
        "    if not is_listy(yb): yb = [yb]\n",
        "    \n",
        "    xb = [tf.constant(v.cpu().numpy()) for v in xb]\n",
        "    yb = [tf.constant(v.cpu().numpy()) for v in yb]\n",
        "    \n",
        "    \n",
        "    if not loss_fn:\n",
        "        out = model(*xb)\n",
        "        out = cb_handler.on_loss_begin(out)\n",
        "        return out[0],yb[0]\n",
        "        \n",
        "    \n",
        "    if opt is not None:\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = model(*xb)\n",
        "            out = cb_handler.on_loss_begin(out)\n",
        "            loss = loss_fn(*yb, out) #reversed params compared to pytorch\n",
        "            mets = [f(*yb, out).numpy() for f in metrics] if metrics is not None else [] #note metrics params order\n",
        "            loss = cb_handler.on_backward_begin(loss)\n",
        "            \n",
        "        \n",
        "        grads = tape.gradient(loss, model.weights)\n",
        "        cb_handler.on_backward_end()\n",
        "        opt.apply_gradients(zip(grads, model.weights))\n",
        "        cb_handler.on_step_end()\n",
        "        \n",
        "    else:\n",
        "        out = model(*xb)\n",
        "        out = cb_handler.on_loss_begin(out)\n",
        "        loss = loss_fn(*yb, out) #reversed params compared to pytorch\n",
        "        mets = [f(*yb, out).numpy() for f in metrics] if metrics is not None else [] #note metrics params order\n",
        "        loss = cb_handler.on_backward_begin(loss)\n",
        "    \n",
        "    return ((loss.numpy(),) + tuple(mets))\n",
        "    \n",
        "def get_preds(model, dl, pbar=None, cb_handler=None):\n",
        "    \"Predict the output of the elements in the dataloader.\"\n",
        "    return [np.concatenate(o) for o in zip(*validate(model, dl, pbar=pbar, cb_handler=cb_handler, average=False))]\n",
        "    \n",
        "    \n",
        "def validate(model, dl, loss_fn=None, metrics=None, cb_handler=None, pbar=None, average=True):\n",
        "    \"Calculate loss and metrics for the validation set.\"\n",
        "    \n",
        "    val_metrics,nums = [],[]\n",
        "    for xb,yb in progress_bar(dl, parent=pbar, leave=(pbar is not None)):\n",
        "        if cb_handler: xb, yb = cb_handler.on_batch_begin(xb, yb, train=False)\n",
        "        val_metrics.append(loss_batch(model, xb, yb, loss_fn, cb_handler=cb_handler, metrics=metrics))\n",
        "        if not is_listy(yb): yb = [yb]\n",
        "        nums.append(yb[0].shape[0])\n",
        "        if cb_handler and cb_handler.on_batch_end(val_metrics[0]): break\n",
        "    nums = np.array(nums, dtype=np.float32)\n",
        "    if average: return [(np.stack(val) * nums).sum() / nums.sum() for val in zip(*val_metrics)]\n",
        "    else: return val_metrics\n",
        "    #return zip(*[loss_batch(model, xb, yb, loss_fn, cb_handler=cb_handler, metrics=metrics)\n",
        "                       #for xb,yb in progress_bar(dl, parent=pbar, leave=(pbar is not None))])\n",
        "\n",
        "\n",
        "def train_epoch(model, dl, opt, loss_func):\n",
        "    \"Simple training of `model` for 1 epoch of `dl` using optim `opt` and loss function `loss_func`.\"\n",
        "    for xb,yb in dl:\n",
        "        if not is_listy(xb): xb = [xb]\n",
        "        if not is_listy(yb): yb = [yb]\n",
        "\n",
        "        xb = [tf.constant(v.cpu().numpy()) for v in xb]\n",
        "        yb = [tf.constant(v.cpu().numpy()) for v in yb]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = model(*xb)\n",
        "            loss = loss_fn(*yb, out) #reversed params compared to pytorch\n",
        "\n",
        "\n",
        "        grads = tape.gradient(loss, model.weights)\n",
        "        opt.apply_gradients(zip(grads, model.weights))\n",
        "\n",
        "def fit(epochs, model, loss_fn, opt, data, callbacks, metrics):\n",
        "    cb_handler = CallbackHandler(callbacks)\n",
        "    pbar = master_bar(range(epochs))\n",
        "    cb_handler.on_train_begin(epochs, pbar=pbar, metrics=metrics)\n",
        "\n",
        "    exception=False\n",
        "    try:\n",
        "        for epoch in pbar:\n",
        "            cb_handler.on_epoch_begin()\n",
        "\n",
        "            for xb,yb in progress_bar(data.train_dl, parent=pbar):\n",
        "                xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
        "                loss = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
        "                if cb_handler.on_batch_end(loss): break\n",
        "\n",
        "            if hasattr(data,'valid_dl') and data.valid_dl is not None:\n",
        "                val_metrics = validate(model, data.valid_dl, loss_fn=loss_fn,\n",
        "                                       cb_handler=cb_handler, metrics=metrics,pbar=pbar)\n",
        "\n",
        "            else: val_metrics=None\n",
        "            if cb_handler.on_epoch_end(val_metrics): break\n",
        "    except Exception as e:\n",
        "        exception = e\n",
        "        raise e\n",
        "    finally: cb_handler.on_train_end(exception)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MlziNoa1PYiJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TFLearner():\n",
        "    \"Train `model` using `data` to minimize `loss_fn` with optimizer `opt_fn`.\"\n",
        "    data:DataBunch\n",
        "    model:tf.keras.Model\n",
        "    opt_fn:Callable\n",
        "    loss_fn:Callable\n",
        "    metrics:Collection[Callable]=None\n",
        "    true_wd:bool=True\n",
        "    bn_wd:bool=True\n",
        "    wd:Floats=default_wd\n",
        "    train_bn:bool=True\n",
        "    path:str=None\n",
        "    model_dir:str='models'\n",
        "    callback_fns:Collection[Callable]=None\n",
        "    callbacks:Collection[Callback]=field(default_factory=list)\n",
        "    layer_groups:Collection[tf.keras.layers.Layer]=None\n",
        "    def __post_init__(self)->None:\n",
        "        \"Setup path,metrics, callbacks and ensure model directory exists.\"\n",
        "        self.path = Path(ifnone(self.path, self.data.path))\n",
        "        (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n",
        "        self.metrics=listify(self.metrics)\n",
        "        if not self.layer_groups: self.layer_groups = flatten_model_tf(self.model)\n",
        "        self.callbacks = listify(self.callbacks)\n",
        "        self.callback_fns = [Recorder] + [Regularizer] + listify(self.callback_fns)\n",
        "        \n",
        "        #build the model by running 1 batch\n",
        "        xb, yb = next(iter(self.data.train_dl))\n",
        "        loss_batch(self.model, xb, yb)\n",
        "\n",
        "    def init(self, init): raise NotImplementedError\n",
        "\n",
        "    def lr_range(self, lr:Union[float,slice])->np.ndarray:\n",
        "        \"Build differential learning rates.\"\n",
        "        if not isinstance(lr,slice): return lr\n",
        "        if lr.start: res = even_mults(lr.start, lr.stop, len(self.layer_groups))\n",
        "        else: res = [lr.stop/3]*(len(self.layer_groups)-1) + [lr.stop]\n",
        "        return np.array(res)\n",
        "\n",
        "    def fit(self, epochs:int, lr:Union[Floats,slice]=default_lr,\n",
        "            wd:Floats=None, callbacks:Collection[Callback]=None)->None:\n",
        "        \"Fit the model on this learner with `lr` learning rate, `wd` weight decay for `epochs` with `callbacks`.\"\n",
        "        lr = self.lr_range(lr)\n",
        "        if wd is None: wd = self.wd\n",
        "        self.create_opt(lr, wd)\n",
        "        callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)      \n",
        "        fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n",
        "            callbacks=self.callbacks+callbacks)\n",
        "\n",
        "    def create_opt(self, lr:Floats, wd:Floats=0.)->None:\n",
        "        \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n",
        "        self.opt = TFOptimWrapper.create(self.opt_fn, lr, wd, self.layer_groups)\n",
        "\n",
        "\n",
        "    def freeze_to(self, n:int)->None:\n",
        "        \"Freeze layers up to layer `n`.\"\n",
        "        for l in self.layer_groups[:n]: \n",
        "            \n",
        "            if not self.train_bn or not isinstance(l, bn_types): l.trainable = False\n",
        "                \n",
        "        for l in self.layer_groups[n:]: l.trainable = True\n",
        "\n",
        "    def freeze(self)->None:\n",
        "        \"Freeze up to last layer.\"\n",
        "        assert(len(self.layer_groups)>1)\n",
        "        self.freeze_to(-1)\n",
        "\n",
        "    def unfreeze(self):\n",
        "        \"Unfreeze entire model.\"\n",
        "        self.freeze_to(0)\n",
        "\n",
        "    def __del__(self): del(self.model, self.data)\n",
        "\n",
        "    def save(self, name:PathOrStr):\n",
        "        \"Save model with `name` to `self.model_dir`.\"\n",
        "        root = tf.train.Checkpoint(model=self.model)\n",
        "        root.save(file_prefix=self.path/self.model_dir/f'{name}')\n",
        "\n",
        "\n",
        "    def load(self, name:PathOrStr):\n",
        "        \"Load model `name` from `self.model_dir`.\"\n",
        "        root = tf.train.Checkpoint(model=self.model)\n",
        "        root.restore(str(self.path/self.model_dir/f'{name}-1'))\n",
        "    \n",
        "    def get_preds(self, is_test=False):\n",
        "        return get_preds(self.model, self.data.holdout(is_test), cb_handler=CallbackHandler(self.callbacks))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bS-efWUZiV1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TFLearner.fit_one_cycle = fit_one_cycle\n",
        "TFLearner.lr_find = lr_find"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iz-RAgm_PYiK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Optim Wrapper"
      ]
    },
    {
      "metadata": {
        "id": "rCI8CgKePYiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO: implement other hyperparams\n",
        "class TFOptimWrapper():\n",
        "    def __init__(self, opt_fn, layer_groups):\n",
        "        self.layer_groups = layer_groups\n",
        "        self._lr = [tfe.Variable(0.0) for o in layer_groups]\n",
        "        self._mom = tfe.Variable(0.0)\n",
        "        self._wd = 0.0\n",
        "        \n",
        "        \n",
        "        opt_params = inspect.signature(opt_fn).parameters\n",
        "        params = {}\n",
        "        if opt_params.get(\"momentum\"):\n",
        "            self.mom = opt_params.get(\"momentum\").default\n",
        "            params[\"momentum\"] = self._mom\n",
        "        if opt_params.get(\"beta1\"):\n",
        "            self.mom = opt_params.get(\"beta1\").default\n",
        "            params[\"beta1\"] = self._mom\n",
        "        \n",
        "        \n",
        "        self.opt = [opt_fn(learning_rate=o, **params) for o in self._lr]\n",
        "        \n",
        "        \n",
        "    @classmethod\n",
        "    def create(cls, opt_fn, lr, wd, layer_groups, **kwargs):\n",
        "        opt = cls(opt_fn, layer_groups,  **kwargs)\n",
        "        \n",
        "        \n",
        "        opt.lr = lr\n",
        "        opt.wd = wd\n",
        "        return opt\n",
        "    \n",
        "        \n",
        "    def apply_gradients(self, grads_and_vars):\n",
        "        for gv, opt, l in zip(grads_and_vars, self.opt, self.layer_groups):\n",
        "            if l.trainable: opt.apply_gradients([gv])\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def lr(self)->float:\n",
        "        \"Get learning rate.\"\n",
        "        return self._lr[-1].numpy()\n",
        "\n",
        "    @lr.setter\n",
        "    def lr(self, val:float)->None:\n",
        "        \"Set learning rate.\"\n",
        "        val = listify(val, self._lr)\n",
        "        for o, v in zip(self._lr, val): o.assign(v) \n",
        "\n",
        "    @property\n",
        "    def mom(self)->float:\n",
        "        \"Get momentum.\"\n",
        "        return self._mom.numpy()\n",
        "\n",
        "    @mom.setter\n",
        "    def mom(self, val:float)->None:\n",
        "        \"Set momentum.\"\n",
        "        if not isinstance(val, float): val = val[-1]\n",
        "        self._mom.assign(val)\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def wd(self)->float:\n",
        "        \"Get wd.\"\n",
        "        return self._wd\n",
        "\n",
        "    @wd.setter\n",
        "    def wd(self, val:float)->None:\n",
        "        \"Set wd.\"\n",
        "        self._wd = val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utBRmuyyZ9kA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ]
    },
    {
      "metadata": {
        "id": "pzkJClb7aAYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Regularizer(LearnerCallback):\n",
        "    def __init__(self, learn:Learner):\n",
        "        super().__init__(learn)\n",
        "    #true_wd=False\n",
        "    def on_backward_begin(self, last_loss, **kwargs):\n",
        "        if not self.learn.true_wd:\n",
        "            regularizer = sum([tf.nn.l2_loss(w) for w in self.learn.model.weights])\n",
        "            return last_loss + self.learn.wd * regularizer\n",
        "    #true_wd=True\n",
        "    def on_backward_end(self, **kwargs):\n",
        "        if self.learn.true_wd:\n",
        "            opt = self.learn.opt\n",
        "            for lr, l in zip(opt._lr, opt.layer_groups):\n",
        "                if l.trainable:\n",
        "                    if self.learn.bn_wd or not isinstance(l, tf_bn_types):\n",
        "                        for w in l.weights: w=w*lr*opt.wd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iU9kDQ3MPYiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ]
    },
    {
      "metadata": {
        "id": "zp5TZ0OMPYiN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_PATH = Path('./')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rt8G8H_4PYiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt_fn = tf.train.AdamOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPVC8tPvPYiR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_fn = tf.losses.sparse_softmax_cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8LHmlYUPYiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def categorical_accuracy_fastai(y_true, y_pred):\n",
        "    return tf.keras.backend.mean(tf.keras.backend.equal(y_true, tf.keras.backend.argmax(y_pred, axis=-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZiBOhoWPYiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics = [categorical_accuracy_fastai]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9t5SHjDwPYiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST with tf.keras.Model"
      ]
    },
    {
      "metadata": {
        "id": "xy3Kzo_zPYiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gzip, pickle\n",
        "MNIST_PATH = DATA_PATH/'mnist'\n",
        "\n",
        "with gzip.open(MNIST_PATH/'mnist.pkl.gz', 'rb') as f:\n",
        "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "\n",
        "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "train_dl = DataLoader(train_ds, 64)\n",
        "valid_dl = DataLoader(valid_ds, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d81v0k3qPYia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = DataBunch(train_dl, valid_dl, path=MNIST_PATH, device=torch.device('cpu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oryCjfdJPYic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mnist_CNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, kernel_size=3, strides=(2,2), padding='same')\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(16, kernel_size=3, strides=(2,2), padding='same')\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "        self.conv3 = tf.keras.layers.Conv2D(10, kernel_size=3, strides=(2,2), padding='same')\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "    def call(self, xb):\n",
        "        xb = tf.reshape(xb, (-1, 1, 28, 28))\n",
        "        xb = tf.nn.relu(self.bn1(self.conv1(xb)))\n",
        "        xb = tf.nn.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = tf.nn.relu(self.bn3(self.conv3(xb)))\n",
        "        xb = tf.nn.pool(xb, (4,4), 'AVG', 'VALID', data_format=\"NCHW\")\n",
        "        xb = tf.reshape(xb, (-1, 10))\n",
        "        return xb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w68rISbqPYih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Mnist_CNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1GDMSKgtPYik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = TFLearner(data, model, opt_fn, loss_fn, metrics=metrics, true_wd=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raiVcblJPYil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZmV9hTNGR7H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit(10, wd=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GVUpFoBFPYiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JCmV71rAPYir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot_lr(show_moms=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIb_UKBJPYit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5E0cIZiNPYiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save(\"test-mnist\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGj6zkkkPYix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load(\"test-mnist\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJ07YBIJPYiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CIFAR with Keras functional API"
      ]
    },
    {
      "metadata": {
        "id": "AcinEP5YPYiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.vision import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooqx_TkgPYi0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CIFAR_PATH = DATA_PATH/\"cifar10\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mv2kiw0APYi2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds_tfms = ([pad(padding=4), crop(size=32, row_pct=(0,1), col_pct=(0,1)), flip_lr(p=0.5)], [])\n",
        "data = image_data_from_folder(CIFAR_PATH, valid='test', ds_tfms=ds_tfms, tfms=cifar_norm, bs=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwLPgNmfPYi3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(data.train_dl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MO1jx5nCPYi5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xb.shape, yb.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EegCFIQ1PYi6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from https://github.com/raghakot/keras-resnet\n",
        "import six\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "CHANNEL_AXIS = 1\n",
        "ROW_AXIS = 2\n",
        "COL_AXIS = 3\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        #x = Permute((3,2,1))(input)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnASjR_zPYi7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.image_data_format()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQzHiKdXPYi9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resnet = ResnetBuilder.build_resnet_50((3, 32 ,32), 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wu5o9qFsPYi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = TFLearner(data, resnet, opt_fn, loss_fn, metrics=metrics, bn_wd=True, train_bn=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlf4m4NVm2U8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQagy_RYhwp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "learn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_JnMnvCsuqRu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot_metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOIv_aMIuy8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B9dBy18Ac-mg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Other"
      ]
    }
  ]
}